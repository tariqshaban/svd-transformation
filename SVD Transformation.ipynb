{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "ug_h_1nCCAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spaCy's Deutsch trained pipelines\n",
        "!python -m spacy download nl_core_news_sm\n",
        "\n",
        "import json\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nl_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from scipy import spatial\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download NLTK's stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download compressed assets from Google Drive\n",
        "!gdown --id 1TTddIx7Bwwl2o3hYnMKDXxJEskiPMXde\n",
        "!unrar x \"assets.rar\"\n",
        "\n",
        "clear_output()\n",
        "print('Successfuly downloaded dependencies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekeVHsTNB36E",
        "outputId": "98787307-5319-4788-9803-9a809e6108fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfuly downloaded dependencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enumerations"
      ],
      "metadata": {
        "id": "omoy7uRgvQ2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating vectorization techniques"
      ],
      "metadata": {
        "id": "wWfeqfTpCoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer(Enum):\n",
        "    TF_IDF_VECTORIZER = 'TfidfVectorizer'\n",
        "    COUNT_VECTORIZER = 'CountVectorizer'\n",
        "    HASHING_VECTORIZER = 'HashingVectorizer'"
      ],
      "metadata": {
        "id": "gsFaBndYCmDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating similarity metrics"
      ],
      "metadata": {
        "id": "4Hrtw79xvNHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity(Enum):\n",
        "    COSINE_SIMILARITY_PAIRWISE = 'cosine_similarity'\n",
        "    COSINE_SIMILARITY = 'cosine'"
      ],
      "metadata": {
        "id": "sGhjF9nnvNXY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating distance metrics"
      ],
      "metadata": {
        "id": "LQGTOsCVvNvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distance(Enum):\n",
        "    EUCLIDEAN_DISTANCE = 'norm'\n",
        "    MANHATTAN_DISTANCE = 'minkowski'\n",
        "    MINKOWSKI_DISTANCE = 'abs_sum'"
      ],
      "metadata": {
        "id": "sVavznk5vOEx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating correlation/accuracy metrics"
      ],
      "metadata": {
        "id": "kjW3GUyu_xMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Correlation(Enum):\n",
        "    PEARSON_CORRELATION = 'pearsonr'    \n",
        "    SPEARMAN_CORRELATION = 'spearmanr'\n",
        "    POINT_BISERIAL_CORRELATION = 'pointbiserialr'\n",
        "    KENDALL_TAU_CORRELATION = 'kendalltau'"
      ],
      "metadata": {
        "id": "emUfokzt_xiB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods"
      ],
      "metadata": {
        "id": "2L9ghTzFFZWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting tables into a dictionary of dataframes"
      ],
      "metadata": {
        "id": "eIij0f7yC9x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_train_test() -> dict:\n",
        "    training = pd.read_csv('assets/training_pairs.csv')\n",
        "    testing = pd.read_csv('assets/testing_pairs.csv')\n",
        "\n",
        "    return {'train': training, 'test': testing}"
      ],
      "metadata": {
        "id": "DiTlAFV7C7cU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching textual data from the residual json assets"
      ],
      "metadata": {
        "id": "eOV99WgyDHoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_json_text_by_id(file_id: str) -> str:\n",
        "    try:\n",
        "        file = open(f'assets/webpages/{file_id}.json')\n",
        "        data = json.load(file)\n",
        "        return data['text']\n",
        "    except FileNotFoundError:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "kyJ5AkFeDI5M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing dataframe"
      ],
      "metadata": {
        "id": "vcGbsENrDY6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df: pd.DataFrame):\n",
        "    # Retrieves textual data by pair_id\n",
        "    df['Text1'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[0]))\n",
        "    df['Text2'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[1]))\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df.drop(df.columns.difference(['Text1', 'Text2', 'Overall']), axis=1, inplace=True)\n",
        "\n",
        "    # Remove null & empty texts\n",
        "    df['Text1'].replace('', None, inplace=True)\n",
        "    df['Text2'].replace('', None, inplace=True)\n",
        "    df.dropna(subset=['Text1', 'Text2'], inplace=True)"
      ],
      "metadata": {
        "id": "fK12Wl4kDZr8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing basic natural language processing procedures\n",
        "* Removing Punctuation\n",
        "* Removing stops words for both Deutsch and English\n",
        "* Removing escape sequences\n",
        "* Lowercasing all characters\n",
        "* Lemmatizing words"
      ],
      "metadata": {
        "id": "gPzBqSIqDnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_nlp(df: pd.DataFrame, efficient: bool=True):\n",
        "    remove_punctuation(df)\n",
        "    remove_stop_words(df)\n",
        "    remove_escape_sequences(df)\n",
        "    lowercase_characters(df)\n",
        "    if not efficient:\n",
        "      lemmatize_words(df)"
      ],
      "metadata": {
        "id": "mdi5jWcRDo18"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    punctuation = list(string.punctuation)\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in punctuation]))"
      ],
      "metadata": {
        "id": "Q41CqsvBE1gG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(df: pd.DataFrame):\n",
        "    stop_words_de = stopwords.words('dutch')\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_de]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_en]))\n"
      ],
      "metadata": {
        "id": "CbEg4faUE2Bl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_escape_sequences(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in escapes]))"
      ],
      "metadata": {
        "id": "J7KisFLvE6Qd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_characters(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].str.lower()"
      ],
      "metadata": {
        "id": "AzuTF8yNE6y9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(df: pd.DataFrame):\n",
        "    lemma_de = nl_core_news_sm.load()\n",
        "    lemma_en = en_core_web_sm.load()\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_de(row)]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_en(row)]))"
      ],
      "metadata": {
        "id": "xxZKKbGGhbK5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting textual data into an array of vectors based on a predefined vectorization technique"
      ],
      "metadata": {
        "id": "BD9Z7T-vES1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(df: pd.DataFrame, columns=None, method: Vectorizer = Vectorizer.TF_IDF_VECTORIZER):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    if method == Vectorizer.TF_IDF_VECTORIZER:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif method == Vectorizer.COUNT_VECTORIZER:\n",
        "        vectorizer = CountVectorizer()\n",
        "    else:\n",
        "        vectorizer = HashingVectorizer()\n",
        "        print('HashingVectorizer appears to have a much higher time complexity due to its high memory usage.')\n",
        "\n",
        "    texts = []\n",
        "    for column in columns:\n",
        "        texts.extend(df[column])\n",
        "\n",
        "    x = vectorizer.fit_transform(texts)\n",
        "    texts_vectorized = x.toarray().tolist()\n",
        "\n",
        "    count = 1\n",
        "    for column in columns:\n",
        "        df[f'Vector{count}'] = texts_vectorized[:len(df[column])]\n",
        "        texts_vectorized = texts_vectorized[len(df[column]) - 1:]\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "t_8KLW6MEShu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating similarity between two vectors"
      ],
      "metadata": {
        "id": "nPVQbTaVvpOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(df: pd.DataFrame, x='Vector1', y='Vector2',\n",
        "                         method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> int:\n",
        "    if method == Similarity.COSINE_SIMILARITY_PAIRWISE:\n",
        "        similarity = cosine_similarity(df[x].tolist(), df[y].tolist()).diagonal().mean()\n",
        "    else:\n",
        "        similarities = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            similarities = np.append(similarities, 1 - spatial.distance.cosine(row[x], row[y]))\n",
        "        similarity = similarities.mean()\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "nj3tv5TvvpmB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating distance between two vectors"
      ],
      "metadata": {
        "id": "LFIuZUZ2vxOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(df: pd.DataFrame, x='Vector1', y='Vector2',\n",
        "                       method: Distance = Distance.EUCLIDEAN_DISTANCE) -> int:\n",
        "    if method == Distance.EUCLIDEAN_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            distances = np.append(distances, np.linalg.norm(np.subtract(row[x], row[y])))\n",
        "        distance = distances.mean()\n",
        "    elif method == Distance.MINKOWSKI_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            distances = np.append(distances, spatial.distance.minkowski(row[x], row[y], 3))\n",
        "        distance = distances.mean()\n",
        "    else:\n",
        "        distances = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            distances = np.append(distances, np.abs(np.subtract(row[x], row[y])).sum())\n",
        "        distance = distances.mean()\n",
        "\n",
        "    return distance"
      ],
      "metadata": {
        "id": "PThSZc-qvxe4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating correlation between two vectors"
      ],
      "metadata": {
        "id": "S_XOToVIBA-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation(df: pd.DataFrame, x='Vector1', y='Vector2',\n",
        "                         method: Correlation = Correlation.PEARSON_CORRELATION) -> int:\n",
        "    if method == Correlation.PEARSON_CORRELATION:\n",
        "        correlations = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            correlations = np.append(correlations, stats.pearsonr(row[x], row[y])[0])\n",
        "        correlation = correlations.mean()\n",
        "    elif method == Correlation.SPEARMAN_CORRELATION:\n",
        "        correlations = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            correlations = np.append(correlations, stats.spearmanr(row[x], row[y])[0])\n",
        "        correlation = correlations.mean()\n",
        "    elif method == Correlation.POINT_BISERIAL_CORRELATION:\n",
        "        correlations = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            correlations = np.append(correlations, stats.pointbiserialr(row[x], row[y])[0])\n",
        "        correlation = correlations.mean()\n",
        "    else: \n",
        "        correlations = np.array([])\n",
        "        for index, row in df.iterrows():\n",
        "            correlations = np.append(correlations, stats.kendalltau(row[x], row[y])[0])\n",
        "        correlation = correlations.mean()\n",
        "\n",
        "    return correlation"
      ],
      "metadata": {
        "id": "Rp-0B0lcBAhK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods invocation"
      ],
      "metadata": {
        "id": "FAT6SfoyElSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oO0zYOBy_K",
        "outputId": "fa9741af-b8ac-4829-8101-0911549a773f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "0.0006639005589506881\n",
            "0.0039032062658923418\n",
            "0.0006639005589506881\n",
            "0.0038942992644011827\n",
            "\n",
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.002990701607000213\n",
            "0.0029907016070002137\n",
            "\n",
            "Distance\n",
            "-----------------------------------------------------------------------\n",
            "1.4120822060068667\n",
            "0.6899844692837535\n",
            "20.639931576108392\n"
          ]
        }
      ],
      "source": [
        "train_test = get_raw_train_test()\n",
        "\n",
        "train = train_test['train']\n",
        "test = train_test['test']\n",
        "\n",
        "preprocess_df(train)\n",
        "preprocess_df(test)\n",
        "\n",
        "implement_nlp(train, efficient=True)\n",
        "implement_nlp(test, efficient=True)\n",
        "\n",
        "vectorize_text(train)\n",
        "vectorize_text(test)\n",
        "\n",
        "print('Correlation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(train, method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(train, method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(train, method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(train, method=Correlation.KENDALL_TAU_CORRELATION))\n",
        "\n",
        "print('\\nSimilarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_similarity(train, method=Similarity.COSINE_SIMILARITY_PAIRWISE))\n",
        "print(calculate_similarity(train, method=Similarity.COSINE_SIMILARITY))\n",
        "\n",
        "print('\\nDistance')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_distance(train, method=Distance.EUCLIDEAN_DISTANCE))\n",
        "print(calculate_distance(train, method=Distance.MINKOWSKI_DISTANCE))\n",
        "print(calculate_distance(train, method=Distance.MANHATTAN_DISTANCE))"
      ]
    }
  ]
}