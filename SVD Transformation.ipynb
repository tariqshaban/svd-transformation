{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "ug_h_1nCCAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spaCy's Deutsch trained pipelines\n",
        "!python -m spacy download nl_core_news_sm\n",
        "\n",
        "import json\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import nl_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from scipy import stats\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download NLTK's stopwords\n",
        "nltk.download('stopwords')\n",
        "# Download NLTK's tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download compressed assets from Google Drive\n",
        "!gdown --id 1TTddIx7Bwwl2o3hYnMKDXxJEskiPMXde\n",
        "!unrar x \"assets.rar\"\n",
        "\n",
        "clear_output()\n",
        "print('Successfuly downloaded dependencies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekeVHsTNB36E",
        "outputId": "eb6dcfa3-90ba-487a-8e21-79b417fa16a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfuly downloaded dependencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "id": "0_Lg5B-7naMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names_training: list = []\n",
        "feature_names_testing: list = []\n",
        "is_legacy: bool = False"
      ],
      "metadata": {
        "id": "lh79UiJWnZWT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enumerations"
      ],
      "metadata": {
        "id": "omoy7uRgvQ2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating vectorization techniques"
      ],
      "metadata": {
        "id": "wWfeqfTpCoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer(Enum):\n",
        "    TF_IDF_VECTORIZER = 'TfidfVectorizer_LEGACY'\n",
        "    COUNT_VECTORIZER = 'CountVectorizer_LEGACY'\n",
        "    HASHING_VECTORIZER = 'HashingVectorizer_LEGACY'\n",
        "    DOC2VEC = 'doc2vec'\n",
        "    BERT = 'bert'\n",
        "    INFER_SENT = 'InferSent'\n",
        "    UNIVERSAL_SENTENCE_ENCODER = 'universal-sentence-encoder'"
      ],
      "metadata": {
        "id": "gsFaBndYCmDc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating similarity metrics"
      ],
      "metadata": {
        "id": "4Hrtw79xvNHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity(Enum):\n",
        "    COSINE_SIMILARITY_PAIRWISE = 'cosine_similarity'\n",
        "    COSINE_SIMILARITY = 'cosine'"
      ],
      "metadata": {
        "id": "sGhjF9nnvNXY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating distance metrics"
      ],
      "metadata": {
        "id": "LQGTOsCVvNvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distance(Enum):\n",
        "    EUCLIDEAN_DISTANCE = 'norm'\n",
        "    MANHATTAN_DISTANCE = 'minkowski'\n",
        "    MINKOWSKI_DISTANCE = 'abs_sum'"
      ],
      "metadata": {
        "id": "sVavznk5vOEx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating correlation/accuracy metrics"
      ],
      "metadata": {
        "id": "kjW3GUyu_xMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Correlation(Enum):\n",
        "    PEARSON_CORRELATION = 'pearsonr'    \n",
        "    SPEARMAN_CORRELATION = 'spearmanr'\n",
        "    POINT_BISERIAL_CORRELATION = 'pointbiserialr'\n",
        "    KENDALL_TAU_CORRELATION = 'kendalltau'"
      ],
      "metadata": {
        "id": "emUfokzt_xiB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods"
      ],
      "metadata": {
        "id": "2L9ghTzFFZWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting tables into a dictionary of dataframes"
      ],
      "metadata": {
        "id": "eIij0f7yC9x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_train_test() -> dict:\n",
        "    training = pd.read_csv('assets/training_pairs.csv')\n",
        "    testing = pd.read_csv('assets/testing_pairs.csv')\n",
        "    \n",
        "\n",
        "    # Redefining train-test split since training data should have the highest count number of 'Overall'\n",
        "    training_overhead = training[training['Overall']<2]\n",
        "    testing_overhead = testing[testing['Overall']==4]\n",
        "    row_relocating_thresh = min(len(training_overhead),len(testing_overhead))\n",
        "\n",
        "    training_overhead = training_overhead.head(row_relocating_thresh)\n",
        "    testing_overhead = testing_overhead.head(row_relocating_thresh)\n",
        "\n",
        "    training = pd.merge(training,training_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    testing = pd.merge(testing,testing_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    training = pd.concat([training, testing_overhead], ignore_index=True)\n",
        "    testing = pd.concat([testing, training_overhead], ignore_index=True)\n",
        "\n",
        "    return {'train': training, 'test': testing}"
      ],
      "metadata": {
        "id": "DiTlAFV7C7cU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching textual data from the residual json assets"
      ],
      "metadata": {
        "id": "eOV99WgyDHoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_json_text_by_id(file_id: str) -> str:\n",
        "    try:\n",
        "        file = open(f'assets/webpages/{file_id}.json')\n",
        "        data = json.load(file)\n",
        "        return data['text']\n",
        "    except FileNotFoundError:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "kyJ5AkFeDI5M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing dataframe"
      ],
      "metadata": {
        "id": "vcGbsENrDY6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df: pd.DataFrame):\n",
        "    # Retrieves textual data by pair_id\n",
        "    df['Text1'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[0]))\n",
        "    df['Text2'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[1]))\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df.drop(df.columns.difference(['Text1', 'Text2', 'Overall']), axis=1, inplace=True)\n",
        "\n",
        "    # Remove null & empty texts\n",
        "    df['Text1'].replace('', None, inplace=True)\n",
        "    df['Text2'].replace('', None, inplace=True)\n",
        "    df.dropna(subset=['Text1', 'Text2'], inplace=True)"
      ],
      "metadata": {
        "id": "fK12Wl4kDZr8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing basic natural language processing procedures\n",
        "* Removing Punctuation\n",
        "* Removing stops words for both Deutsch and English\n",
        "* Removing escape sequences\n",
        "* Lowercasing all characters\n",
        "* Lemmatizing words"
      ],
      "metadata": {
        "id": "gPzBqSIqDnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_nlp(df: pd.DataFrame, efficient: bool=True):\n",
        "    remove_punctuation(df)\n",
        "    remove_stop_words(df)\n",
        "    remove_escape_sequences(df)\n",
        "    lowercase_characters(df)\n",
        "    if not efficient:\n",
        "      lemmatize_words(df)"
      ],
      "metadata": {
        "id": "mdi5jWcRDo18"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    punctuation = list(string.punctuation)\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in punctuation]))"
      ],
      "metadata": {
        "id": "Q41CqsvBE1gG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(df: pd.DataFrame):\n",
        "    stop_words_de = stopwords.words('dutch')\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_de]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_en]))\n"
      ],
      "metadata": {
        "id": "CbEg4faUE2Bl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_escape_sequences(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in escapes]))"
      ],
      "metadata": {
        "id": "J7KisFLvE6Qd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_characters(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].str.lower()"
      ],
      "metadata": {
        "id": "AzuTF8yNE6y9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(df: pd.DataFrame):\n",
        "    lemma_de = nl_core_news_sm.load()\n",
        "    lemma_en = en_core_web_sm.load()\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_de(row)]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_en(row)]))"
      ],
      "metadata": {
        "id": "xxZKKbGGhbK5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting textual data into an array of vectors based on a predefined vectorization technique"
      ],
      "metadata": {
        "id": "BD9Z7T-vES1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(df: pd.DataFrame, columns=None, method: Vectorizer = Vectorizer.TF_IDF_VECTORIZER,\n",
        "                   is_training: bool = True):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    texts = []\n",
        "    for column in columns:\n",
        "        texts.extend(df[column])\n",
        "\n",
        "    if method == Vectorizer.TF_IDF_VECTORIZER:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif method == Vectorizer.COUNT_VECTORIZER:\n",
        "        vectorizer = CountVectorizer()\n",
        "    elif method == Vectorizer.HASHING_VECTORIZER:\n",
        "        vectorizer = HashingVectorizer()\n",
        "        print('HashingVectorizer appears to have a much higher time complexity due to its high memory usage.')\n",
        "    elif method == Vectorizer.DOC2VEC:\n",
        "        tokenized_sent = []\n",
        "        for s in texts:\n",
        "            tokenized_sent.append(word_tokenize(s.lower()))\n",
        "        tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
        "        model = Doc2Vec(tagged_data, vector_size=50)\n",
        "        # texts_vectorized = model.dv.vectors.tolist()\n",
        "        texts_vectorized = model.docvecs.vectors_docs.tolist()\n",
        "    elif method == Vectorizer.BERT:\n",
        "        raise Exception('Not Implemented')\n",
        "    elif method == Vectorizer.INFER_SENT:\n",
        "        raise Exception('Not Implemented')\n",
        "    else:\n",
        "        raise Exception('Not Implemented')\n",
        "\n",
        "    if 'LEGACY' in method.value:\n",
        "        x = vectorizer.fit_transform(texts)\n",
        "        if is_training:\n",
        "            global feature_names_training\n",
        "            feature_names_training = vectorizer.get_feature_names_out()\n",
        "        else:\n",
        "            global feature_names_testing\n",
        "            feature_names_testing = vectorizer.get_feature_names_out()\n",
        "        texts_vectorized = x.toarray().tolist()\n",
        "\n",
        "    global is_legacy\n",
        "    if 'LEGACY' in method.value:\n",
        "        is_legacy = True\n",
        "    else:\n",
        "        is_legacy = False\n",
        "\n",
        "    count = 1\n",
        "    for column in columns:\n",
        "        df[f'Vector{count}'] = texts_vectorized[:len(df[column])]\n",
        "        texts_vectorized = texts_vectorized[len(df[column]) - 1:]\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "t_8KLW6MEShu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating similarity between two vectors"
      ],
      "metadata": {
        "id": "nPVQbTaVvpOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(x: list, y: list, method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> list:\n",
        "    if method == Similarity.COSINE_SIMILARITY_PAIRWISE:\n",
        "        if not isinstance(x, list):\n",
        "            x = x.tolist()\n",
        "        if not isinstance(y, list):\n",
        "            y = y.tolist()\n",
        "        similarity = cosine_similarity(x, y).diagonal()\n",
        "    else:\n",
        "        similarity = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            similarity = np.append(similarity, 1 - spatial.distance.cosine(x_vector, y_vector))\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "nj3tv5TvvpmB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating distance between two vectors"
      ],
      "metadata": {
        "id": "LFIuZUZ2vxOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(x: list, y: list, method: Distance = Distance.EUCLIDEAN_DISTANCE) -> int:\n",
        "    if method == Distance.EUCLIDEAN_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.linalg.norm(np.subtract(x_vector, y_vector)))\n",
        "        distance = distances.mean()\n",
        "    elif method == Distance.MINKOWSKI_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, spatial.distance.minkowski(x_vector, y_vector, 3))\n",
        "        distance = distances.mean()\n",
        "    else:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.abs(np.subtract(x_vector, y_vector)).sum())\n",
        "        distance = distances.mean()\n",
        "\n",
        "    return distance"
      ],
      "metadata": {
        "id": "PThSZc-qvxe4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating correlation between two vectors"
      ],
      "metadata": {
        "id": "S_XOToVIBA-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation(x: list, y: list, z: list, method: Correlation = Correlation.PEARSON_CORRELATION,\n",
        "                          similarity_method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> int:\n",
        "    similarity_vec = calculate_similarity(x, y, method=similarity_method)\n",
        "\n",
        "    if method == Correlation.PEARSON_CORRELATION:\n",
        "        correlation = stats.pearsonr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.SPEARMAN_CORRELATION:\n",
        "        correlation = stats.spearmanr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.POINT_BISERIAL_CORRELATION:\n",
        "        correlation = stats.pointbiserialr(z, similarity_vec)[0]\n",
        "    else:\n",
        "        correlation = stats.kendalltau(z, similarity_vec)[0]\n",
        "\n",
        "    return correlation"
      ],
      "metadata": {
        "id": "Rp-0B0lcBAhK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalizing list"
      ],
      "metadata": {
        "id": "ud6rNb9aY-yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(a, axis=-1, order=2):\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)"
      ],
      "metadata": {
        "id": "c-sU0KbAY-ii"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement SVD transformation"
      ],
      "metadata": {
        "id": "ZTNw5KgMY7C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True, sigma_percentage=0.85):\n",
        "    source_matrix = np.array(source_matrix.tolist())\n",
        "    target_matrix = np.array(target_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalize(source_matrix)\n",
        "        target_matrix = normalize(target_matrix)\n",
        "\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    u, s, v = np.linalg.svd(product, full_matrices=False)\n",
        "\n",
        "    threshold = np.percentile(s, sigma_percentage * 100)\n",
        "    top = len(s[s > threshold])\n",
        "\n",
        "    u = u[:, :top]\n",
        "    v = v[:top, :]\n",
        "\n",
        "    return np.matmul(u, v)"
      ],
      "metadata": {
        "id": "JO1ikyhCY6Ta"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_transformation_single_matrix(source_matrix, target_matrix, normalize_vectors=True, sigma_percentage=0.85):\n",
        "    source_matrix = np.array(source_matrix.tolist())\n",
        "    target_matrix = np.array(target_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalize(source_matrix)\n",
        "        target_matrix = normalize(target_matrix)\n",
        "\n",
        "    single_matrix = np.append(source_matrix, target_matrix, axis=0)\n",
        "\n",
        "    u, s, v = np.linalg.svd(single_matrix, full_matrices=False)\n",
        "\n",
        "    threshold = np.percentile(s, sigma_percentage * 100)\n",
        "    top = len(s[s > threshold])\n",
        "\n",
        "    u = u[:, :top]\n",
        "    v = v[:top, :]\n",
        "\n",
        "    return np.matmul(u, v)"
      ],
      "metadata": {
        "id": "K4kIK-TfZQwq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Equates test dimensions with training dimensions for legacy vectorization techniques"
      ],
      "metadata": {
        "id": "DcoeN_sZ8swQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equate_test_dimensions_legacy(test_df: pd.DataFrame, x='Vector1', y='Vector2'):\n",
        "    training_columns = pd.DataFrame(columns=list(feature_names_training))\n",
        "\n",
        "    test_df_vector_1 = pd.DataFrame(test_df[x].values.tolist(), columns=feature_names_testing)\n",
        "    test_df_vector_2 = pd.DataFrame(test_df[y].values.tolist(), columns=feature_names_testing)\n",
        "\n",
        "    test_df_vector_1 = pd.concat([test_df_vector_1, training_columns])\n",
        "    test_df_vector_1.drop(columns=list(set(feature_names_testing) - set(feature_names_training)), inplace=True)\n",
        "    test_df_vector_1.fillna(0, inplace=True)\n",
        "\n",
        "    test_df_vector_2 = pd.concat([test_df_vector_2, training_columns])\n",
        "    test_df_vector_2.drop(columns=list(set(feature_names_testing) - set(feature_names_training)), inplace=True)\n",
        "    test_df_vector_2.fillna(0, inplace=True)\n",
        "\n",
        "    test_df[x] = test_df_vector_1.values.tolist()\n",
        "    test_df[y] = test_df_vector_2.values.tolist()"
      ],
      "metadata": {
        "id": "p4YUcLKX8saD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test SVD transformation on new data"
      ],
      "metadata": {
        "id": "trgCz2ZplEuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformation(test_vector, transform):\n",
        "    test_vector = np.array(test_vector)\n",
        "\n",
        "    return np.matmul(transform, test_vector)"
      ],
      "metadata": {
        "id": "azK9l7L0lFAi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformation_iterable(test_matrix, transform) -> list:\n",
        "    transformed_matrix = []\n",
        "\n",
        "    for vector in test_matrix:\n",
        "        transformed_matrix.append(apply_transformation(vector, transform))\n",
        "\n",
        "    return transformed_matrix"
      ],
      "metadata": {
        "id": "1G5FtguaBmy0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods invocation"
      ],
      "metadata": {
        "id": "FAT6SfoyElSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Required procedures to conduct SVD"
      ],
      "metadata": {
        "id": "s10JqUh6MmiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test = get_raw_train_test()\n",
        "\n",
        "train = train_test['train']\n",
        "test = train_test['test']\n",
        "\n",
        "preprocess_df(train)\n",
        "preprocess_df(test)\n",
        "\n",
        "implement_nlp(train, efficient=True)\n",
        "implement_nlp(test, efficient=True)\n",
        "\n",
        "vectorize_text(train, method=Vectorizer.DOC2VEC, is_training=True)\n",
        "vectorize_text(test, method=Vectorizer.DOC2VEC, is_training=False)\n",
        "\n",
        "if is_legacy:\n",
        "    equate_test_dimensions_legacy(test)\n",
        "    transformation = learn_transformation_single_matrix(train['Vector1'], train['Vector2'])\n",
        "else:\n",
        "    transformation = learn_transformation(train['Vector1'], train['Vector2'])\n",
        "\n",
        "transformed1 = apply_transformation_iterable(test['Vector1'], transformation)\n",
        "transformed2 = apply_transformation_iterable(test['Vector2'], transformation)"
      ],
      "metadata": {
        "id": "cHFUoLsuMmHd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics before transformation"
      ],
      "metadata": {
        "id": "Ar5OtyfBM-T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nDistance')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_distance(train['Vector1'], train['Vector2'],\n",
        "                         method=Distance.EUCLIDEAN_DISTANCE))\n",
        "print(calculate_distance(train['Vector1'], train['Vector2'],\n",
        "                         method=Distance.MINKOWSKI_DISTANCE))\n",
        "print(calculate_distance(train['Vector1'], train['Vector2'],\n",
        "                         method=Distance.MANHATTAN_DISTANCE))\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ],
      "metadata": {
        "id": "jcxNfUfiNI4D",
        "outputId": "aa22b94e-d6b7-4886-feb2-94974f60c52d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.7419996334950566\n",
            "0.7419996334950566\n",
            "\n",
            "Distance\n",
            "-----------------------------------------------------------------------\n",
            "1.412547684477733\n",
            "0.8446017003241897\n",
            "8.099387468330951\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.022138990780000034\n",
            "0.01467953681024551\n",
            "-0.022138990780000034\n",
            "0.01171652728058726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics after transformation"
      ],
      "metadata": {
        "id": "iTGM-KbTMwE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oO0zYOBy_K",
        "outputId": "459dbdec-880d-4575-80b1-cc9020a6ef17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.9978315513837818\n",
            "0.9978315513837818\n",
            "\n",
            "Distance\n",
            "-----------------------------------------------------------------------\n",
            "0.9689178110736759\n",
            "0.573759893440179\n",
            "5.634728736853833\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.11803380165277659\n",
            "-0.33772422689217374\n",
            "-0.11803380165277659\n",
            "-0.2641341076583875\n"
          ]
        }
      ],
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(transformed1, transformed2,\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(transformed1, transformed2,\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nDistance')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_distance(transformed1, transformed2,\n",
        "                         method=Distance.EUCLIDEAN_DISTANCE))\n",
        "print(calculate_distance(transformed1, transformed2,\n",
        "                         method=Distance.MINKOWSKI_DISTANCE))\n",
        "print(calculate_distance(transformed1, transformed2,\n",
        "                         method=Distance.MANHATTAN_DISTANCE))\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(transformed1, transformed2, test['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(transformed1, transformed2, test['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(transformed1, transformed2, test['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(transformed1, transformed2, test['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ]
    }
  ]
}