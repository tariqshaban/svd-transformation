{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Dependencies"
      ],
      "metadata": {
        "id": "ug_h_1nCCAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download NLTK's stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download compressed assets from Google Drive\n",
        "!gdown --id 1TTddIx7Bwwl2o3hYnMKDXxJEskiPMXde\n",
        "!unrar x \"assets.rar\"\n",
        "\n",
        "clear_output()\n",
        "print('Successfuly downloaded dependencies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekeVHsTNB36E",
        "outputId": "9ede3cab-a61e-48af-e2cf-bef42fca085e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfuly downloaded dependencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enumerating vectorization techniques"
      ],
      "metadata": {
        "id": "wWfeqfTpCoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer(Enum):\n",
        "    TF_IDF_VECTORIZER = 'TfidfVectorizer'\n",
        "    COUNT_VECTORIZER = 'CountVectorizer'\n",
        "    HASHING_VECTORIZER = 'HashingVectorizer'"
      ],
      "metadata": {
        "id": "gsFaBndYCmDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting tables into a dictionary of dataframes"
      ],
      "metadata": {
        "id": "eIij0f7yC9x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_train_test() -> dict:\n",
        "    training = pd.read_csv('assets/training_pairs.csv')\n",
        "    testing = pd.read_csv('assets/testing_pairs.csv')\n",
        "\n",
        "    return {'train': training, 'test': testing}"
      ],
      "metadata": {
        "id": "DiTlAFV7C7cU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching textual data from the residual json assets"
      ],
      "metadata": {
        "id": "eOV99WgyDHoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_json_text_by_id(file_id: str) -> str:\n",
        "    try:\n",
        "        file = open(f'assets/webpages/{file_id}.json')\n",
        "        data = json.load(file)\n",
        "        return data['text']\n",
        "    except FileNotFoundError:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "kyJ5AkFeDI5M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing dataframe"
      ],
      "metadata": {
        "id": "vcGbsENrDY6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df: pd.DataFrame):\n",
        "    # Retrieves textual data by pair_id\n",
        "    df['Text1'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[0]))\n",
        "    df['Text2'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[1]))\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df.drop(df.columns.difference(['Text1', 'Text2', 'Overall']), axis=1, inplace=True)\n",
        "\n",
        "    # Remove null & empty texts\n",
        "    df['Text1'].replace('', None, inplace=True)\n",
        "    df['Text2'].replace('', None, inplace=True)\n",
        "    df.dropna(subset=['Text1', 'Text2'], inplace=True)"
      ],
      "metadata": {
        "id": "fK12Wl4kDZr8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing basic natural language processing procedures\n",
        "* Removing Punctuation\n",
        "* Removing stops words for both Deutsch and English\n",
        "* Removing escape sequences\n",
        "* Lowercasing all characters"
      ],
      "metadata": {
        "id": "gPzBqSIqDnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_nlp(df: pd.DataFrame):\n",
        "    remove_punctuation(df)\n",
        "    remove_stop_words(df)\n",
        "    remove_escape_sequences(df)\n",
        "    lowercase_characters(df)"
      ],
      "metadata": {
        "id": "mdi5jWcRDo18"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    punctuation = list(string.punctuation)\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in punctuation]))"
      ],
      "metadata": {
        "id": "Q41CqsvBE1gG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(df: pd.DataFrame):\n",
        "    stop_words_de = stopwords.words('dutch')\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_de]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_en]))"
      ],
      "metadata": {
        "id": "CbEg4faUE2Bl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_escape_sequences(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in escapes]))"
      ],
      "metadata": {
        "id": "J7KisFLvE6Qd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_characters(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].str.lower()"
      ],
      "metadata": {
        "id": "AzuTF8yNE6y9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting textual data into an array of vectors based on a predefined vectorization technique"
      ],
      "metadata": {
        "id": "BD9Z7T-vES1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(df: pd.DataFrame, columns=None, method: Vectorizer = Vectorizer.TF_IDF_VECTORIZER):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    if method == Vectorizer.TF_IDF_VECTORIZER:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "    elif method == Vectorizer.COUNT_VECTORIZER:\n",
        "        vectorizer = CountVectorizer()\n",
        "    else:\n",
        "        vectorizer = HashingVectorizer()\n",
        "        print('HashingVectorizer appears to have a much higher time complexity due to its high memory usage.')\n",
        "\n",
        "    count = 1\n",
        "    for column in columns:\n",
        "        x = vectorizer.fit_transform(df[column])\n",
        "        df[f'Vector{count}'] = x.toarray().tolist()\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "t_8KLW6MEShu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Methods invocation"
      ],
      "metadata": {
        "id": "FAT6SfoyElSt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oO0zYOBy_K",
        "outputId": "d591c025-c0b0-4750-da22-a22b4cc7d9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['kommentare zum artikel bitte beachten sie beim verfassen eines kommentars regeln höflicher kommunikation viola crell 03042020 1049 tom erste entschuldigung wenn ich mich einmische aber das ministerium berlin hat ihre hilfe wirklich abgelehnt wenn sie das schriftlich haben dann sollten sie das wirklich veröffentlichen tom erste 03042020 0716 hatte ich vergessen wäre das nicht auch ein artikel für fw redaktion tom erste 02042020 2108 oliver hilgendorff sie dürfen davon ausgehen vorn rollo´s runter kneipe auf privatgrundstück kamera an hintertür und gut ist wenn sich ein grüner bürgermeister aus berlin aus solidarität mit seiner freundin absichtlich ansteckt wenn trotz warnungen import von sogenannten asylbewerbern bei nacht und nebel ungestört weitergeht und wenn gewisse großfamilien bei ihren ausflügen von polizei und ordnungsamt tunlichst ruhe gelassen werden dann kann es auch gar nicht so übel aussehen freiwillige unterstützung ist vom bundesgesundheitsamt offenbar auch nicht gewünscht ich bin gelernter fernsehfritze und hatte schon anfang voriger woche per mail angefragt ob ich irgendwie helfen kann montage oder elektronik von beatmungsgeräten oder dergleichen auch heimarbeit hatte um eine diesbezügliche kontaktadresse gebeten aber bis heute keine antwort die haben es einfach nicht nötig aber das hänge ich noch an große glocke diese gurkentruppe ist nur auf eigene reputation bedacht also warum soll ich dann nicht meine kneipe gehen oliver hilgendorff 02042020 1647 tom erste darf ich davon ausgehen daß sie ganz gemütlich ihrem wirtshaus sitzen und mit ihren kumpanen bechern tom erste 02042020 0754 und mittlerweile läuft mein privater filmdienst auf hochtouren der absolute renner ist defa 1947 das mir von meinen saufkumpanen keiner glauben wollte daß es da sogar einen film gibt hatte ich schon mehrere wetten gewonnen angela klassen kampf 01042020 2345 vielleicht werden mutti und corona auch chronisch wenigstens muss das klopapier dann nicht mehr riesengroßen lagern vorgehalten werden weils ne lieferzeit von zehn jahren hat aber bitte nicht morgens liefern da kommt schon neue kühlschrank sam lowry 01042020 2331 ich habe genug klopapier und desi weil ich nicht mehr gelesen hab sondern bei dirk müller ihr seid dumm brot britta 01042020 1940 regt euch wegen klopapier nicht auf ein supermarkt so hörte ich verkauft sogar klopapier mit ostermotiven ha ha ha na dann frohe ostern willi winzig 01042020 1210 ja das mit dem prüfstand da ist dran hier einmal 7 punkte welche angegangen werden sollte um zu retten noch zu retten ist vor allem unsere freiheiten unser bargeld und unsere unabhängigkeit von fremdbestimmung 1 die gesamte derzeitige regierung inkl linker und grüner müssen sofort quarantäne und zwar so dass hinter stahlstäben gesiebte luft atmen müssen nachdem sie für ihre vielen gesetzesbrüche rechtskräftig verurteilt wurden 2 dier gesamt hirnrissige energieproduktion über wind und sonne muss sofort auf ein sinnvolles mindestmaß gekappt werden und akws müssen wieder sofort ans netz soweit sie noch zu aktivieren sind wenn es nicht reicht um preiswerte energie für wirtschaft und gesellschaft zu liefern muss kohle und gas wieder dazu 3 die steuerverschwendung für fiktive asylanten besser invasoren und wirtschaftsflüchtlinge muss sofort gestoppt werden und grenzen müssen dicht gemacht werden besonders für dieses klientel dazu kommen sog rettungspläne für marode banken maroden staaten nicht schuldenmachen sondern sparen ist angesagt 4 deutschland sollte sofort den dexit einleiten denn auch eu ganoven haben sich völlig unfähig erwiesen um auch nur einen vernünftigen vorschlag zu machen geschweige denn vernünftig zu handeln 5 die nwostaatsmedien müssen zerschlagen werden und freier journalismus sollte an deren stelle treten 6 der ausufernde parlamentarismus allen bereichen bis zu den kommunen muss wieder zurück gestutzt werden damit krake linken und grünen ideologen sich nicht wieder einmischen und mehltau jede eigeninitiative blockieren um deren spinnerten ideologien willen 7 das gesamte schul und universitätssystem muss von grund auf reformiert werden damit kinder wieder für ihre leben lernen und universitäten dem fortschritt sachlichen wissenschaften dienen und nicht momentan völlig bescheuerten ideologien für minderheiten und durch geknallte spinner']\n",
            "['kindly share this story the world’s largest coffee chain starbucks corp sbuxo stopped accepting reusable cups customers prevent spread coronavirus though still honor promised discount anyone carrying one companies across world rethink way operate slow spread virus first emerged china opting freeze travel stockpile goods staff work home for starbucks means pausing use personal cups tumblers stores “of abundance caution” coffee chains recent years encouraged customers bring reusable cups thermal tumblers cut number paper cups plastic lids end landfill read also first case coronavirus reported vatican many offer discounts increased loyalty stamps promote trend “we continue honour existing discounts anyone brings personal cup” starbucks said “as result suspending 5p uk 5c germany charges paper cups well given decision prevents customers opting reusables” vanguard kindly share this story']\n",
            "32933\n",
            "15921\n"
          ]
        }
      ],
      "source": [
        "train_test = get_raw_train_test()\n",
        "\n",
        "train = train_test['train']\n",
        "test = train_test['test']\n",
        "\n",
        "preprocess_df(train)\n",
        "preprocess_df(test)\n",
        "\n",
        "implement_nlp(train)\n",
        "implement_nlp(test)\n",
        "\n",
        "print(test['Text1'].head(1).tolist())\n",
        "print(test['Text2'].head(1).tolist())\n",
        "\n",
        "vectorize_text(train)\n",
        "vectorize_text(test)\n",
        "\n",
        "print(len(train['Vector1'][0]))\n",
        "print(len(train['Vector2'][0]))"
      ]
    }
  ]
}