{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqshaban/svd-transformation/blob/master/SVD%20Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "ug_h_1nCCAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spaCy's Deutsch trained pipelines\n",
        "!python -m spacy download nl_core_news_sm\n",
        "\n",
        "# Install sentence-transformers, for implementing SentenceBERT\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Install Facebook's InferSent supervised sentence embedding technique\n",
        "import os\n",
        "!mkdir -p assets/infer_sent/encoder\n",
        "!curl -Lo assets/infer_sent/encoder/model.py https://raw.githubusercontent.com/facebookresearch/InferSent/main/models.py\n",
        "!curl -Lo assets/infer_sent/encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "!mkdir -p assets/infer_sent/glove\n",
        "!pip install kaggle\n",
        "!gdown --id 1tFeb9OTQH0T_CUmWin8DkcrYwZP0ao8y\n",
        "!chmod 600 kaggle.json\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
        "!kaggle datasets download -d gerwynng/glove-common-crawl-840b-tokens\n",
        "!mv glove-common-crawl-840b-tokens.zip /content/assets/infer_sent/glove\n",
        "!unzip assets/infer_sent/glove/glove-common-crawl-840b-tokens.zip -d assets/infer_sent/glove/\n",
        "\n",
        "# Enabling colored terminal text for warnings\n",
        "!pip install colorama\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import nl_core_news_sm\n",
        "import en_core_web_sm\n",
        "import torch\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from scipy import stats\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import clear_output\n",
        "from assets.infer_sent.encoder.model import InferSent\n",
        "\n",
        "# Download NLTK's stopwords\n",
        "nltk.download('stopwords')\n",
        "# Download NLTK's tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download compressed assets from Google Drive\n",
        "!gdown --id 1TTddIx7Bwwl2o3hYnMKDXxJEskiPMXde\n",
        "!unrar x \"assets.rar\"\n",
        "\n",
        "clear_output()\n",
        "print('Successfully downloaded dependencies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekeVHsTNB36E",
        "outputId": "6f97594a-eae9-4fe8-f1cc-eb23db18b56a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded dependencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enumerations"
      ],
      "metadata": {
        "id": "omoy7uRgvQ2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating vectorization techniques"
      ],
      "metadata": {
        "id": "wWfeqfTpCoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer(Enum):\n",
        "    TF_IDF_VECTORIZER = 'TfidfVectorizer_LEGACY'\n",
        "    COUNT_VECTORIZER = 'CountVectorizer_LEGACY'\n",
        "    HASHING_VECTORIZER = 'HashingVectorizer_LEGACY_INTENSIVE'\n",
        "    DOC2VEC = 'doc2vec'\n",
        "    BERT = 'bert_INTENSIVE'\n",
        "    INFER_SENT = 'InferSent_INTENSIVE'\n",
        "    UNIVERSAL_SENTENCE_ENCODER = 'universal-sentence-encoder'"
      ],
      "metadata": {
        "id": "gsFaBndYCmDc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating similarity metrics"
      ],
      "metadata": {
        "id": "4Hrtw79xvNHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity(Enum):\n",
        "    COSINE_SIMILARITY_PAIRWISE = 'cosine_similarity'\n",
        "    COSINE_SIMILARITY = 'cosine'"
      ],
      "metadata": {
        "id": "sGhjF9nnvNXY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating distance metrics"
      ],
      "metadata": {
        "id": "LQGTOsCVvNvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distance(Enum):\n",
        "    EUCLIDEAN_DISTANCE = 'norm'\n",
        "    MANHATTAN_DISTANCE = 'minkowski'\n",
        "    MINKOWSKI_DISTANCE = 'abs_sum'"
      ],
      "metadata": {
        "id": "sVavznk5vOEx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating correlation/accuracy metrics"
      ],
      "metadata": {
        "id": "kjW3GUyu_xMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Correlation(Enum):\n",
        "    PEARSON_CORRELATION = 'pearsonr'    \n",
        "    SPEARMAN_CORRELATION = 'spearmanr'\n",
        "    POINT_BISERIAL_CORRELATION = 'pointbiserialr'\n",
        "    KENDALL_TAU_CORRELATION = 'kendalltau'"
      ],
      "metadata": {
        "id": "emUfokzt_xiB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods"
      ],
      "metadata": {
        "id": "2L9ghTzFFZWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting tables into a dictionary of dataframes"
      ],
      "metadata": {
        "id": "eIij0f7yC9x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_train_test() -> dict:\n",
        "    training = pd.read_csv('assets/training_pairs.csv')\n",
        "    testing = pd.read_csv('assets/testing_pairs.csv')\n",
        "    \n",
        "\n",
        "    # Redefining train-test split since training data should have the highest count number of 'Overall'\n",
        "    training_overhead = training[training['Overall']<2]\n",
        "    testing_overhead = testing[testing['Overall']==4]\n",
        "    row_relocating_thresh = min(len(training_overhead),len(testing_overhead))\n",
        "\n",
        "    training_overhead = training_overhead.head(row_relocating_thresh)\n",
        "    testing_overhead = testing_overhead.head(row_relocating_thresh)\n",
        "\n",
        "    training = pd.merge(training,training_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    testing = pd.merge(testing,testing_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    training = pd.concat([training, testing_overhead], ignore_index=True)\n",
        "    testing = pd.concat([testing, training_overhead], ignore_index=True)\n",
        "\n",
        "    return {'train': training, 'test': testing}"
      ],
      "metadata": {
        "id": "DiTlAFV7C7cU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching textual data from the residual json assets"
      ],
      "metadata": {
        "id": "eOV99WgyDHoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_json_text_by_id(file_id: str) -> str:\n",
        "    try:\n",
        "        file = open(f'assets/webpages/{file_id}.json')\n",
        "        data = json.load(file)\n",
        "        return data['text']\n",
        "    except FileNotFoundError:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "kyJ5AkFeDI5M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing dataframe"
      ],
      "metadata": {
        "id": "vcGbsENrDY6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df: pd.DataFrame):\n",
        "    # Retrieves textual data by pair_id\n",
        "    df['Text1'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[0]))\n",
        "    df['Text2'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[1]))\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df.drop(df.columns.difference(['Text1', 'Text2', 'Overall']), axis=1, inplace=True)\n",
        "\n",
        "    # Remove null & empty texts\n",
        "    df['Text1'].replace('', None, inplace=True)\n",
        "    df['Text2'].replace('', None, inplace=True)\n",
        "    df.dropna(subset=['Text1', 'Text2'], inplace=True)"
      ],
      "metadata": {
        "id": "fK12Wl4kDZr8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing basic natural language processing procedures\n",
        "* Removing Punctuation\n",
        "* Removing Numbers\n",
        "* Removing stops words for both Deutsch and English\n",
        "* Removing escape sequences\n",
        "* Lowercasing all characters\n",
        "* Lemmatizing words"
      ],
      "metadata": {
        "id": "gPzBqSIqDnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_nlp(df: pd.DataFrame, efficient: bool=True):\n",
        "    remove_punctuation(df)\n",
        "    remove_numbers(df)\n",
        "    remove_stop_words(df)\n",
        "    remove_escape_sequences(df)\n",
        "    lowercase_characters(df)\n",
        "    if not efficient:\n",
        "      lemmatize_words(df)"
      ],
      "metadata": {
        "id": "mdi5jWcRDo18"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    punctuation = list(string.punctuation)\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in punctuation]))"
      ],
      "metadata": {
        "id": "Q41CqsvBE1gG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: re.sub(r'\\d+', '', row))"
      ],
      "metadata": {
        "id": "DNCfXkSwTfse"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(df: pd.DataFrame):\n",
        "    stop_words_de = stopwords.words('dutch')\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_de]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_en]))\n"
      ],
      "metadata": {
        "id": "CbEg4faUE2Bl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_escape_sequences(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in escapes]))"
      ],
      "metadata": {
        "id": "J7KisFLvE6Qd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_characters(df: pd.DataFrame, columns=None):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].str.lower()"
      ],
      "metadata": {
        "id": "AzuTF8yNE6y9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(df: pd.DataFrame):\n",
        "    lemma_de = nl_core_news_sm.load()\n",
        "    lemma_en = en_core_web_sm.load()\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_de(row)]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_en(row)]))"
      ],
      "metadata": {
        "id": "xxZKKbGGhbK5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting textual data into an array of vectors based on a predefined vectorization technique"
      ],
      "metadata": {
        "id": "BD9Z7T-vES1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(df: pd.DataFrame, columns=None, method: Vectorizer = Vectorizer.TF_IDF_VECTORIZER,\n",
        "                   is_training: bool = True):\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    texts = []\n",
        "    for column in columns:\n",
        "        texts.extend(df[column])\n",
        "\n",
        "    if is_training:\n",
        "        if 'LEGACY' in method.value:\n",
        "            print(Back.YELLOW + Fore.BLACK +\n",
        "                  'A legacy vectorization technique has been selected, expect poor results.')\n",
        "        if 'INTENSIVE' in method.value:\n",
        "            print(Back.YELLOW + Fore.BLACK +\n",
        "                  'A computationally intensive vectorization technique has been selected, expect long runtimes.')\n",
        "        print(Style.RESET_ALL)\n",
        "\n",
        "    if method == Vectorizer.TF_IDF_VECTORIZER:\n",
        "        vectorizer = TfidfVectorizer(max_features=4000)\n",
        "    elif method == Vectorizer.COUNT_VECTORIZER:\n",
        "        vectorizer = CountVectorizer(max_features=4000)\n",
        "    elif method == Vectorizer.HASHING_VECTORIZER:\n",
        "        vectorizer = HashingVectorizer()\n",
        "    elif method == Vectorizer.DOC2VEC:\n",
        "        tokenized_sent = []\n",
        "        for s in texts:\n",
        "            tokenized_sent.append(word_tokenize(s.lower()))\n",
        "        tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
        "        model = Doc2Vec(tagged_data, vector_size=2000)\n",
        "        texts_vectorized = model.docvecs.vectors_docs.tolist()\n",
        "    elif method == Vectorizer.BERT:\n",
        "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "        texts_vectorized = model.encode(texts, show_progress_bar=True).tolist()\n",
        "    elif method == Vectorizer.INFER_SENT:\n",
        "        params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                        'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
        "        model = InferSent(params_model)\n",
        "        model.load_state_dict(torch.load('assets/infer_sent/encoder/infersent2.pkl'))\n",
        "        model.set_w2v_path('assets/infer_sent/glove/glove.840B.300d.txt')\n",
        "        model.build_vocab(texts, tokenize=True)\n",
        "        texts_vectorized = model.encode(texts).tolist()\n",
        "    else:\n",
        "        model = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')\n",
        "        texts_vectorized = model(texts).numpy().tolist()\n",
        "        \n",
        "    if 'LEGACY' in method.value:\n",
        "        x = vectorizer.fit_transform(texts)\n",
        "        texts_vectorized = x.toarray().tolist()\n",
        "\n",
        "    count = 1\n",
        "    for column in columns:\n",
        "        df[f'Vector{count}'] = texts_vectorized[:len(df[column])]\n",
        "        texts_vectorized = texts_vectorized[len(df[column]) - 1:]\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "t_8KLW6MEShu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating similarity between two vectors"
      ],
      "metadata": {
        "id": "nPVQbTaVvpOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(x: list, y: list, method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> list:\n",
        "    if method == Similarity.COSINE_SIMILARITY_PAIRWISE:\n",
        "        if not isinstance(x, list):\n",
        "            x = x.tolist()\n",
        "        if not isinstance(y, list):\n",
        "            y = y.tolist()\n",
        "        similarity = cosine_similarity(x, y).diagonal()\n",
        "    else:\n",
        "        similarity = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            similarity = np.append(similarity, 1 - spatial.distance.cosine(x_vector, y_vector))\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "nj3tv5TvvpmB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating distance between two vectors"
      ],
      "metadata": {
        "id": "LFIuZUZ2vxOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(x: list, y: list, method: Distance = Distance.EUCLIDEAN_DISTANCE) -> int:\n",
        "    if method == Distance.EUCLIDEAN_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.linalg.norm(np.subtract(x_vector, y_vector)))\n",
        "        distance = distances.mean()\n",
        "    elif method == Distance.MINKOWSKI_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, spatial.distance.minkowski(x_vector, y_vector, 3))\n",
        "        distance = distances.mean()\n",
        "    else:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.abs(np.subtract(x_vector, y_vector)).sum())\n",
        "        distance = distances.mean()\n",
        "\n",
        "    return distance"
      ],
      "metadata": {
        "id": "PThSZc-qvxe4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating correlation between two vectors"
      ],
      "metadata": {
        "id": "S_XOToVIBA-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation(x: list, y: list, z: list, method: Correlation = Correlation.PEARSON_CORRELATION,\n",
        "                          similarity_method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> int:\n",
        "    similarity_vec = calculate_similarity(x, y, method=similarity_method)\n",
        "\n",
        "    if method == Correlation.PEARSON_CORRELATION:\n",
        "        correlation = stats.pearsonr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.SPEARMAN_CORRELATION:\n",
        "        correlation = stats.spearmanr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.POINT_BISERIAL_CORRELATION:\n",
        "        correlation = stats.pointbiserialr(z, similarity_vec)[0]\n",
        "    else:\n",
        "        correlation = stats.kendalltau(z, similarity_vec)[0]\n",
        "\n",
        "    return correlation"
      ],
      "metadata": {
        "id": "Rp-0B0lcBAhK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalizing list"
      ],
      "metadata": {
        "id": "ud6rNb9aY-yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(a, axis=-1, order=2):\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)"
      ],
      "metadata": {
        "id": "c-sU0KbAY-ii"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement SVD transformation"
      ],
      "metadata": {
        "id": "ZTNw5KgMY7C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True, sigma_percentage=0.85):\n",
        "    source_matrix = np.array(source_matrix.tolist())\n",
        "    target_matrix = np.array(target_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalize(source_matrix)\n",
        "        target_matrix = normalize(target_matrix)\n",
        "\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    u, s, v = np.linalg.svd(product, full_matrices=False)\n",
        "\n",
        "    threshold = np.percentile(s, sigma_percentage * 100)\n",
        "    top = len(s[s > threshold])\n",
        "\n",
        "    u = u[:, :top]\n",
        "    s = np.diag(s[:top])\n",
        "    v = v[:top, :]\n",
        "\n",
        "    return u.dot(s).dot(v)"
      ],
      "metadata": {
        "id": "JO1ikyhCY6Ta"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test SVD transformation on new data"
      ],
      "metadata": {
        "id": "trgCz2ZplEuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformation(test_matrix, transform, normalize_vectors=True) -> list:\n",
        "    test_matrix = np.array(test_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        test_matrix = normalize(test_matrix)\n",
        "\n",
        "    return list(np.matmul(test_matrix, transform))"
      ],
      "metadata": {
        "id": "azK9l7L0lFAi"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods invocation"
      ],
      "metadata": {
        "id": "FAT6SfoyElSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Required procedures to conduct SVD"
      ],
      "metadata": {
        "id": "s10JqUh6MmiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test = get_raw_train_test()\n",
        "\n",
        "train = train_test['train']\n",
        "test = train_test['test']\n",
        "\n",
        "preprocess_df(train)\n",
        "preprocess_df(test)\n",
        "\n",
        "implement_nlp(train, efficient=True)\n",
        "implement_nlp(test, efficient=True)\n",
        "\n",
        "vectorize_text(train, method=Vectorizer.DOC2VEC, is_training=True)\n",
        "vectorize_text(test, method=Vectorizer.DOC2VEC, is_training=False)\n",
        "\n",
        "transformation = learn_transformation(train['Vector1'], train['Vector2'])\n",
        "\n",
        "transformed = apply_transformation(test['Vector2'], transformation)"
      ],
      "metadata": {
        "id": "cHFUoLsuMmHd",
        "outputId": "9c6a66fc-30d2-4983-b2cc-c447fa3328e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics before transformation"
      ],
      "metadata": {
        "id": "Ar5OtyfBM-T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ],
      "metadata": {
        "id": "jcxNfUfiNI4D",
        "outputId": "5962c2c1-4e4b-4fd3-d3c4-577391bbfbf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.8215781356008606\n",
            "0.8215781356008598\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.017819942099382646\n",
            "0.012036311109871538\n",
            "-0.017819942099382646\n",
            "0.009681819214493959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics after transformation"
      ],
      "metadata": {
        "id": "iTGM-KbTMwE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oO0zYOBy_K",
        "outputId": "7a89b9e2-5a70-4297-b7a2-4291711b842d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.6736916110927281\n",
            "0.6736916110927279\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.21681078735974119\n",
            "-0.3085397114584499\n",
            "-0.21681078735974119\n",
            "-0.24076839813476097\n"
          ]
        }
      ],
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(transformed, test['Vector1'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(transformed, test['Vector1'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ]
    }
  ]
}