{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVD Transformation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tariqshaban/svd-transformation/blob/master/SVD%20Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dependencies"
      ],
      "metadata": {
        "id": "ug_h_1nCCAOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spaCy's Deutsch trained pipelines\n",
        "!python -m spacy download nl_core_news_sm\n",
        "\n",
        "# Install sentence-transformers, for implementing SentenceBERT\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Install Facebook's InferSent supervised sentence embedding technique\n",
        "import os\n",
        "!mkdir -p assets/infer_sent/encoder\n",
        "!curl -Lo assets/infer_sent/encoder/model.py https://raw.githubusercontent.com/facebookresearch/InferSent/main/models.py\n",
        "!curl -Lo assets/infer_sent/encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
        "!mkdir -p assets/infer_sent/glove\n",
        "!pip install kaggle\n",
        "!gdown --id 1tFeb9OTQH0T_CUmWin8DkcrYwZP0ao8y\n",
        "!chmod 600 kaggle.json\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
        "!kaggle datasets download -d gerwynng/glove-common-crawl-840b-tokens\n",
        "!mv glove-common-crawl-840b-tokens.zip /content/assets/infer_sent/glove\n",
        "!unzip assets/infer_sent/glove/glove-common-crawl-840b-tokens.zip -d assets/infer_sent/glove/\n",
        "\n",
        "# Required for multilingual universal sentence encoder\n",
        "!pip install tensorflow_text\n",
        "\n",
        "# Enabling colored terminal text for warnings\n",
        "!pip install colorama\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import nl_core_news_sm\n",
        "import en_core_web_sm\n",
        "import torch\n",
        "\n",
        "from colorama import Fore, Back, Style\n",
        "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
        "from enum import Enum\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from scipy import spatial\n",
        "from scipy import stats\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import clear_output\n",
        "from assets.infer_sent.encoder.model import InferSent\n",
        "\n",
        "# Download NLTK's stopwords\n",
        "nltk.download('stopwords')\n",
        "# Download NLTK's tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download compressed assets from Google Drive\n",
        "!gdown --id 1TTddIx7Bwwl2o3hYnMKDXxJEskiPMXde\n",
        "!unrar x \"assets.rar\"\n",
        "\n",
        "clear_output()\n",
        "print('Successfully downloaded dependencies')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekeVHsTNB36E",
        "outputId": "1e83d7a8-abd0-4a24-d6e9-5d868e30f67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded dependencies\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enumerations"
      ],
      "metadata": {
        "id": "omoy7uRgvQ2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating vectorization techniques"
      ],
      "metadata": {
        "id": "wWfeqfTpCoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer(Enum):\n",
        "    \"\"\"\n",
        "    Enumerating vectorization techniques.\n",
        "\n",
        "    Values\n",
        "    -------\n",
        "    **TF_IDF_VECTORIZER:** Convert a collection of raw documents to a matrix of TF-IDF features, see\n",
        "    `documentation <TF_IDF_VECTORIZER_>`_.\n",
        "\n",
        "    **COUNT_VECTORIZER:** Convert a collection of text documents to a matrix of token counts, see\n",
        "    `documentation <COUNT_VECTORIZER_>`_.\n",
        "\n",
        "    **HASHING_VECTORIZER:** Convert a collection of text documents to a matrix of token occurrences, see\n",
        "    `documentation <HASHING_VECTORIZER_>`_.\n",
        "\n",
        "    **DOC2VEC:** Learn document embeddings via the distributed memory and distributed bag of words models, see\n",
        "    `documentation <DOC2VEC_>`_.\n",
        "\n",
        "    **BERT:** Map sentences/text to embeddings using the `bert-base-nli-mean-tokens` pretrained model, see\n",
        "    `documentation <BERT_>`_.\n",
        "\n",
        "    **INFER_SENT:** An implementation that has a semantic representations for sentences, see\n",
        "    `repository <INFER_SENT_>`_.\n",
        "\n",
        "    **UNIVERSAL_SENTENCE_ENCODER:** Encoder of greater-than-word length text trained on a variety of data, see\n",
        "    `documentation <UNIVERSAL_SENTENCE_ENCODER_1_>`_ and `notebook <UNIVERSAL_SENTENCE_ENCODER_2_>`_.\n",
        "\n",
        "    .. _TF_IDF_VECTORIZER: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "    .. _COUNT_VECTORIZER: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "    .. _HASHING_VECTORIZER: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
        "    .. _DOC2VEC: https://radimrehurek.com/gensim/models/doc2vec.html\n",
        "    .. _BERT: https://www.sbert.net\n",
        "    .. _INFER_SENT: https://github.com/facebookresearch/InferSent\n",
        "    .. _UNIVERSAL_SENTENCE_ENCODER_1: https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
        "    .. _UNIVERSAL_SENTENCE_ENCODER_2: https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb\n",
        "    \"\"\"\n",
        "    TF_IDF_VECTORIZER = 'TfidfVectorizer_LEGACY'\n",
        "    COUNT_VECTORIZER = 'CountVectorizer_LEGACY'\n",
        "    HASHING_VECTORIZER = 'HashingVectorizer_LEGACY_INTENSIVE'\n",
        "    DOC2VEC = 'doc2vec'\n",
        "    BERT = 'bert_INTENSIVE'\n",
        "    INFER_SENT = 'InferSent_INTENSIVE'\n",
        "    UNIVERSAL_SENTENCE_ENCODER = 'universal-sentence-encoder'"
      ],
      "metadata": {
        "id": "gsFaBndYCmDc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating similarity metrics"
      ],
      "metadata": {
        "id": "4Hrtw79xvNHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Similarity(Enum):\n",
        "    \"\"\"\n",
        "    Enumerating similarity metrics.\n",
        "\n",
        "    Values\n",
        "    -------\n",
        "    **COSINE_SIMILARITY_PAIRWISE:** Compute cosine similarity between samples in X and Y, see\n",
        "    `documentation <COSINE_SIMILARITY_PAIRWISE_>`_.\n",
        "\n",
        "    **COSINE_SIMILARITY:** Compute the Cosine distance between 1-D arrays, see\n",
        "    `documentation <COSINE_SIMILARITY_>`_.\n",
        "\n",
        "    .. _COSINE_SIMILARITY_PAIRWISE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n",
        "    .. _COSINE_SIMILARITY: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
        "    \"\"\"\n",
        "    COSINE_SIMILARITY_PAIRWISE = 'cosine_similarity'\n",
        "    COSINE_SIMILARITY = 'cosine'"
      ],
      "metadata": {
        "id": "sGhjF9nnvNXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating distance metrics"
      ],
      "metadata": {
        "id": "LQGTOsCVvNvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Distance(Enum):\n",
        "    \"\"\"\n",
        "    Enumerating distance metrics.\n",
        "\n",
        "    Values\n",
        "    -------\n",
        "    **EUCLIDEAN_DISTANCE:** Return one of eight different matrix norms; since the euclidean distance is the l2 norm, see\n",
        "    `documentation <EUCLIDEAN_DISTANCE_>`_.\n",
        "\n",
        "    **MANHATTAN_DISTANCE:** Compute the Manhattan distance between two 1-D arrays, see\n",
        "    `example <MANHATTAN_DISTANCE_>`_.\n",
        "\n",
        "    **MINKOWSKI_DISTANCE:** Compute the Minkowski distance between two 1-D arrays, see\n",
        "    `documentation <MINKOWSKI_DISTANCE_>`_.\n",
        "\n",
        "    .. _EUCLIDEAN_DISTANCE: https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "    .. _MANHATTAN_DISTANCE: https://stackoverflow.com/a/62634895\n",
        "    .. _MINKOWSKI_DISTANCE: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html\n",
        "    \"\"\"\n",
        "    EUCLIDEAN_DISTANCE = 'norm'\n",
        "    MANHATTAN_DISTANCE = 'minkowski'\n",
        "    MINKOWSKI_DISTANCE = 'abs_sum'"
      ],
      "metadata": {
        "id": "sVavznk5vOEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enumerating correlation/accuracy metrics"
      ],
      "metadata": {
        "id": "kjW3GUyu_xMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Correlation(Enum):\n",
        "    \"\"\"\n",
        "    Enumerating correlation/accuracy metrics.\n",
        "\n",
        "    Values\n",
        "    -------\n",
        "    **PEARSON_CORRELATION:** Pearson correlation coefficient and p-value for testing non-correlation, see\n",
        "    `documentation <PEARSON_CORRELATION_>`_.\n",
        "\n",
        "    **SPEARMAN_CORRELATION:** Calculate a Spearman correlation coefficient with associated p-value, see\n",
        "    `documentation <SPEARMAN_CORRELATION_>`_.\n",
        "\n",
        "    **POINT_BISERIAL_CORRELATION:** Calculate a point biserial correlation coefficient and its p-value, see\n",
        "    `documentation <POINT_BISERIAL_CORRELATION_>`_.\n",
        "\n",
        "    **KENDALL_TAU_CORRELATION:** Compute Calculate Kendallâ€™s tau, a correlation measure for ordinal data, see\n",
        "    `documentation <KENDALL_TAU_CORRELATION_>`_.\n",
        "\n",
        "    .. _PEARSON_CORRELATION: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
        "    .. _SPEARMAN_CORRELATION: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
        "    .. _POINT_BISERIAL_CORRELATION: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html\n",
        "    .. _KENDALL_TAU_CORRELATION: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kendalltau.html\n",
        "    \"\"\"\n",
        "    PEARSON_CORRELATION = 'pearsonr'    \n",
        "    SPEARMAN_CORRELATION = 'spearmanr'\n",
        "    POINT_BISERIAL_CORRELATION = 'pointbiserialr'\n",
        "    KENDALL_TAU_CORRELATION = 'kendalltau'"
      ],
      "metadata": {
        "id": "emUfokzt_xiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods"
      ],
      "metadata": {
        "id": "2L9ghTzFFZWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mounting tables into a dictionary of dataframes"
      ],
      "metadata": {
        "id": "eIij0f7yC9x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_raw_train_test() -> dict:\n",
        "    \"\"\"\n",
        "    Loads training and testing CSV pairs into their respective dictionary key as a dataframe.\n",
        "\n",
        "    :return: A dictionary denoting either a training dataframe or a testing dataframe as its values\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    training = pd.read_csv('assets/training_pairs.csv')\n",
        "    testing = pd.read_csv('assets/testing_pairs.csv')\n",
        "    \n",
        "\n",
        "    # Redefining train-test split since training data should have the highest count number of 'Overall'\n",
        "    training_overhead = training[training['Overall']<2]\n",
        "    testing_overhead = testing[testing['Overall']==4]\n",
        "    row_relocating_thresh = min(len(training_overhead),len(testing_overhead))\n",
        "\n",
        "    training_overhead = training_overhead.head(row_relocating_thresh)\n",
        "    testing_overhead = testing_overhead.head(row_relocating_thresh)\n",
        "\n",
        "    training = pd.merge(training,training_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    testing = pd.merge(testing,testing_overhead, indicator=True, how='outer')\\\n",
        "        .query('_merge==\"left_only\"')\\\n",
        "        .drop('_merge', axis=1)\n",
        "\n",
        "    training = pd.concat([training, testing_overhead], ignore_index=True)\n",
        "    testing = pd.concat([testing, training_overhead], ignore_index=True)\n",
        "\n",
        "    return {'train': training, 'test': testing}"
      ],
      "metadata": {
        "id": "DiTlAFV7C7cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fetching textual data from the residual json assets"
      ],
      "metadata": {
        "id": "eOV99WgyDHoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_json_text_by_id(file_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches the value of the 'text' key in a JSON file for a specified id.\n",
        "\n",
        "    :param str file_id: Denotes the file id, in which specifies which existing JSON file to fetch\n",
        "    :return: The 'text' value from the JSON\n",
        "    :rtype: str\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file = open(f'assets/webpages/{file_id}.json')\n",
        "        data = json.load(file)\n",
        "        return data['text']\n",
        "    except FileNotFoundError:\n",
        "        return ''"
      ],
      "metadata": {
        "id": "kyJ5AkFeDI5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing dataframe"
      ],
      "metadata": {
        "id": "vcGbsENrDY6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Adds the text columns and discards unnecessary columns from the dataframe, \n",
        "    drops rows that have null values in text column.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedures on\n",
        "    \"\"\"\n",
        "    # Retrieves textual data by pair_id\n",
        "    df['Text1'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[0]))\n",
        "    df['Text2'] = df['pair_id'].apply(lambda cell: __get_json_text_by_id(cell.split('_')[1]))\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    df.drop(df.columns.difference(['Text1', 'Text2', 'Overall']), axis=1, inplace=True)\n",
        "\n",
        "    # Remove null & empty texts\n",
        "    df['Text1'].replace('', None, inplace=True)\n",
        "    df['Text2'].replace('', None, inplace=True)\n",
        "    df.dropna(subset=['Text1', 'Text2'], inplace=True)"
      ],
      "metadata": {
        "id": "fK12Wl4kDZr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implementing basic natural language processing procedures\n",
        "* Removing Punctuation\n",
        "* Removing Numbers\n",
        "* Removing stops words for both Deutsch and English\n",
        "* Removing escape sequences\n",
        "* Lowercasing all characters\n",
        "* Lemmatizing words"
      ],
      "metadata": {
        "id": "gPzBqSIqDnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_nlp(df: pd.DataFrame, efficient: bool=True):\n",
        "    \"\"\"\n",
        "    Implements basic natural language processing procedures on the text columns.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedures on\n",
        "    :param bool efficient: Specify whether to lemmatize the words or not (CPU intensive)\n",
        "    \"\"\"\n",
        "    remove_punctuation(df)\n",
        "    remove_numbers(df)\n",
        "    remove_stop_words(df)\n",
        "    remove_escape_sequences(df)\n",
        "    lowercase_characters(df)\n",
        "    if not efficient:\n",
        "      lemmatize_words(df)"
      ],
      "metadata": {
        "id": "mdi5jWcRDo18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(df: pd.DataFrame, columns=None):\n",
        "    \"\"\"\n",
        "    Removes the punctuations on selected columns.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    :param list columns: Specify which columns from the dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    punctuation = list(string.punctuation)\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in punctuation]))"
      ],
      "metadata": {
        "id": "Q41CqsvBE1gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(df: pd.DataFrame, columns=None):\n",
        "    \"\"\"\n",
        "    Removes the numbers on selected columns.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    :param list columns: Specify which columns from the dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: re.sub(r'\\d+', '', row))"
      ],
      "metadata": {
        "id": "DNCfXkSwTfse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Removes the stop words on predefined columns, since it is a language-sensitive method.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    stop_words_de = stopwords.words('dutch')\n",
        "    stop_words_en = stopwords.words('english')\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_de]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([i for i in row.split() if i not in stop_words_en]))\n"
      ],
      "metadata": {
        "id": "CbEg4faUE2Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_escape_sequences(df: pd.DataFrame, columns=None):\n",
        "    \"\"\"\n",
        "    Removes the escape sequences on selected columns (\\t, \\n, etc.).\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    :param list columns: Specify which columns from the dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "    for column in columns:\n",
        "        df[column] = df[column].apply(lambda row: ''.join([i for i in row if i not in escapes]))"
      ],
      "metadata": {
        "id": "J7KisFLvE6Qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_characters(df: pd.DataFrame, columns=None):\n",
        "    \"\"\"\n",
        "    Lowercase all alphabetical texts on selected columns.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    :param list columns: Specify which columns from the dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    for column in columns:\n",
        "        df[column] = df[column].str.lower()"
      ],
      "metadata": {
        "id": "AzuTF8yNE6y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_words(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Returns each word to its base or dictionary form on predefined columns, since it is a language-sensitive method.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    \"\"\"\n",
        "    lemma_de = nl_core_news_sm.load()\n",
        "    lemma_en = en_core_web_sm.load()\n",
        "    df['Text1'] = df['Text1'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_de(row)]))\n",
        "    df['Text2'] = df['Text2'].apply(lambda row: ' '.join([x.lemma_ for x in lemma_en(row)]))"
      ],
      "metadata": {
        "id": "xxZKKbGGhbK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting textual data into an array of vectors based on a predefined vectorization technique"
      ],
      "metadata": {
        "id": "BD9Z7T-vES1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(df: pd.DataFrame, columns=None, method: Vectorizer = Vectorizer.TF_IDF_VECTORIZER,\n",
        "                   is_training: bool = True):\n",
        "    \"\"\"\n",
        "    Converts textual data into an array of vectors based on a predefined vectorization technique.\n",
        "\n",
        "    :param pd.DataFrame df: Specify which dataframe to apply the procedure on\n",
        "    :param list columns: Specify which columns from the dataframe to apply the procedure on\n",
        "    :param Vectorizer method: Specify the vectorization method to use\n",
        "    :param bool is_training: Specify whether the dataframe is the training or the testing dataframe\n",
        "    \"\"\"\n",
        "    if columns is None:\n",
        "        columns = ['Text1', 'Text2']\n",
        "\n",
        "    texts = []\n",
        "    for column in columns:\n",
        "        texts.extend(df[column])\n",
        "\n",
        "    if is_training:\n",
        "        if 'LEGACY' in method.value:\n",
        "            print(Back.YELLOW + Fore.BLACK +\n",
        "                  'A legacy vectorization technique has been selected, expect poor results.')\n",
        "        if 'INTENSIVE' in method.value:\n",
        "            print(Back.YELLOW + Fore.BLACK +\n",
        "                  'A computationally intensive vectorization technique has been selected, expect long runtimes.')\n",
        "        print(Style.RESET_ALL)\n",
        "\n",
        "    if method == Vectorizer.TF_IDF_VECTORIZER:\n",
        "        vectorizer = TfidfVectorizer(max_features=4000)\n",
        "    elif method == Vectorizer.COUNT_VECTORIZER:\n",
        "        vectorizer = CountVectorizer(max_features=4000)\n",
        "    elif method == Vectorizer.HASHING_VECTORIZER:\n",
        "        vectorizer = HashingVectorizer()\n",
        "    elif method == Vectorizer.DOC2VEC:\n",
        "        tokenized_sent = []\n",
        "        for s in texts:\n",
        "            tokenized_sent.append(word_tokenize(s.lower()))\n",
        "        tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]\n",
        "        model = Doc2Vec(tagged_data, vector_size=2000)\n",
        "        texts_vectorized = model.docvecs.vectors_docs.tolist()\n",
        "    elif method == Vectorizer.BERT:\n",
        "        model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "        texts_vectorized = model.encode(texts, show_progress_bar=True).tolist()\n",
        "    elif method == Vectorizer.INFER_SENT:\n",
        "        params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
        "                        'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n",
        "        model = InferSent(params_model)\n",
        "        model.load_state_dict(torch.load('assets/infer_sent/encoder/infersent2.pkl'))\n",
        "        model.set_w2v_path('assets/infer_sent/glove/glove.840B.300d.txt')\n",
        "        model.build_vocab(texts, tokenize=True)\n",
        "        texts_vectorized = model.encode(texts).tolist()\n",
        "    else:\n",
        "        model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')\n",
        "        texts_vectorized = []\n",
        "        for column in columns:\n",
        "          texts_vectorized.extend(model(df[column]).numpy().tolist())\n",
        "        \n",
        "    if 'LEGACY' in method.value:\n",
        "        x = vectorizer.fit_transform(texts)\n",
        "        texts_vectorized = x.toarray().tolist()\n",
        "\n",
        "    count = 1\n",
        "    for column in columns:\n",
        "        df[f'Vector{count}'] = texts_vectorized[:len(df[column])]\n",
        "        texts_vectorized = texts_vectorized[len(df[column]):]\n",
        "        count = count + 1"
      ],
      "metadata": {
        "id": "t_8KLW6MEShu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating similarity between two vectors"
      ],
      "metadata": {
        "id": "nPVQbTaVvpOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(x: list, y: list, method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> list:\n",
        "    \"\"\"\n",
        "    Calculates similarity between two matrices.\n",
        "\n",
        "    :param list x: The first matrix\n",
        "    :param list y: The second matrix, that must be equal to 'x' in length\n",
        "    :param Similarity method: Specify the similarity method to use\n",
        "    :return: A list of the similarity score between each vector pair\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if method == Similarity.COSINE_SIMILARITY_PAIRWISE:\n",
        "        if not isinstance(x, list):\n",
        "            x = x.tolist()\n",
        "        if not isinstance(y, list):\n",
        "            y = y.tolist()\n",
        "        similarity = cosine_similarity(x, y).diagonal()\n",
        "    else:\n",
        "        similarity = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            similarity = np.append(similarity, 1 - spatial.distance.cosine(x_vector, y_vector))\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "nj3tv5TvvpmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating distance between two vectors"
      ],
      "metadata": {
        "id": "LFIuZUZ2vxOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(x: list, y: list, method: Distance = Distance.EUCLIDEAN_DISTANCE) -> float:\n",
        "    \"\"\"\n",
        "    Calculates distance between two matrices.\n",
        "\n",
        "    :param list x: The first matrix\n",
        "    :param list y: The second matrix, that must be equal to 'x' in length\n",
        "    :param Distance method: Specify the distance method to use\n",
        "    :return: The mean distance between each vector pair\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    if method == Distance.EUCLIDEAN_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.linalg.norm(np.subtract(x_vector, y_vector)))\n",
        "        distance = distances.mean()\n",
        "    elif method == Distance.MINKOWSKI_DISTANCE:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, spatial.distance.minkowski(x_vector, y_vector, 3))\n",
        "        distance = distances.mean()\n",
        "    else:\n",
        "        distances = np.array([])\n",
        "        for x_vector, y_vector in zip(x, y):\n",
        "            distances = np.append(distances, np.abs(np.subtract(x_vector, y_vector)).sum())\n",
        "        distance = distances.mean()\n",
        "\n",
        "    return distance"
      ],
      "metadata": {
        "id": "PThSZc-qvxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating correlation between two vectors"
      ],
      "metadata": {
        "id": "S_XOToVIBA-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_correlation(x: list, y: list, z: list, method: Correlation = Correlation.PEARSON_CORRELATION,\n",
        "                          similarity_method: Similarity = Similarity.COSINE_SIMILARITY_PAIRWISE) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the correlation between matrix z, and the pairwise similarity of both matrix x and matrix y.\n",
        "\n",
        "    :param list x: The first matrix\n",
        "    :param list y: The second matrix, that must be equal to 'x' in length\n",
        "    :param list z: The assumed true class vector, that must be equal to 'x' in length\n",
        "    :param Correlation method: Specify the correlation method to use\n",
        "    :param Similarity similarity_method: Specify the similarity method to use between matrix x and matrix y\n",
        "    :return: The correlation value between vector z, and the pairwise similarity of both matrix x and matrix y\n",
        "    :rtype: float\n",
        "    \"\"\"\n",
        "    similarity_vec = calculate_similarity(x, y, method=similarity_method)\n",
        "\n",
        "    if method == Correlation.PEARSON_CORRELATION:\n",
        "        correlation = stats.pearsonr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.SPEARMAN_CORRELATION:\n",
        "        correlation = stats.spearmanr(z, similarity_vec)[0]\n",
        "    elif method == Correlation.POINT_BISERIAL_CORRELATION:\n",
        "        correlation = stats.pointbiserialr(z, similarity_vec)[0]\n",
        "    else:\n",
        "        correlation = stats.kendalltau(z, similarity_vec)[0]\n",
        "\n",
        "    return correlation"
      ],
      "metadata": {
        "id": "Rp-0B0lcBAhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalizing list"
      ],
      "metadata": {
        "id": "ud6rNb9aY-yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(a, axis=-1, order=2):\n",
        "    \"\"\"\n",
        "    Normalizes the values of a specified matrix.\n",
        "\n",
        "    :param a: The matrix\n",
        "    :param axis: If `axis` is an integer, it specifies the axis of `x` along which to compute the vector norms.\n",
        "        If `axis` is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices\n",
        "        are computed.  If `axis` is None then either a vector norm (when `x` is 1-D) or a matrix norm (when `x` is 2-D)\n",
        "        is returned. The default is None\n",
        "    :param order: order of the norm\n",
        "    :return: The normalized matrix\n",
        "    :rtype: ndarray\n",
        "    \"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)"
      ],
      "metadata": {
        "id": "c-sU0KbAY-ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement SVD transformation"
      ],
      "metadata": {
        "id": "ZTNw5KgMY7C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True, sigma_percentage=0.85):\n",
        "    \"\"\"\n",
        "    Performs an SVD transformation on two matrices.\n",
        "\n",
        "    :param source_matrix: The matrix that should be projected on the target matrix dimension/domain\n",
        "    :param target_matrix: The target matrix\n",
        "    :param normalize_vectors: Specify whether to normalize the vectors or not\n",
        "    :param sigma_percentage: Specify the sigma percentile in which values below the threshold will be discarded\n",
        "    :return: The transformed matrix\n",
        "    \"\"\"\n",
        "    source_matrix = np.array(source_matrix.tolist())\n",
        "    target_matrix = np.array(target_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        source_matrix = normalize(source_matrix)\n",
        "        target_matrix = normalize(target_matrix)\n",
        "\n",
        "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
        "    u, s, v = np.linalg.svd(product, full_matrices=False)\n",
        "\n",
        "    threshold = np.percentile(s, sigma_percentage * 100)\n",
        "    top = len(s[s > threshold])\n",
        "\n",
        "    u = u[:, :top]\n",
        "    s = np.diag(s[:top])\n",
        "    v = v[:top, :]\n",
        "\n",
        "    return u.dot(s).dot(v)"
      ],
      "metadata": {
        "id": "JO1ikyhCY6Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test SVD transformation on new data"
      ],
      "metadata": {
        "id": "trgCz2ZplEuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transformation(test_matrix, transform, normalize_vectors=True) -> list:\n",
        "    \"\"\"\n",
        "    Project a new test matrix into the SVD.\n",
        "\n",
        "    :param test_matrix: The matrix that should be projected on the target matrix dimension/domain\n",
        "    :param transform: The SVD transformed matrix\n",
        "    :param normalize_vectors: Specify whether to normalize the test_matrix or not\n",
        "    :return: The transformed vector\n",
        "    \"\"\"\n",
        "    test_matrix = np.array(test_matrix.tolist())\n",
        "\n",
        "    if normalize_vectors:\n",
        "        test_matrix = normalize(test_matrix)\n",
        "\n",
        "    return list(np.matmul(test_matrix, transform))"
      ],
      "metadata": {
        "id": "azK9l7L0lFAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods invocation"
      ],
      "metadata": {
        "id": "FAT6SfoyElSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Required procedures to conduct SVD"
      ],
      "metadata": {
        "id": "s10JqUh6MmiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_test = get_raw_train_test()\n",
        "\n",
        "train = train_test['train']\n",
        "test = train_test['test']\n",
        "\n",
        "preprocess_df(train)\n",
        "preprocess_df(test)\n",
        "\n",
        "implement_nlp(train, efficient=True)\n",
        "implement_nlp(test, efficient=True)\n",
        "\n",
        "vectorize_text(train, method=Vectorizer.DOC2VEC, is_training=True)\n",
        "vectorize_text(test, method=Vectorizer.DOC2VEC, is_training=False)\n",
        "\n",
        "transformation = learn_transformation(train['Vector1'], train['Vector2'])\n",
        "\n",
        "transformed = apply_transformation(test['Vector2'], transformation)"
      ],
      "metadata": {
        "id": "cHFUoLsuMmHd",
        "outputId": "f1a3b59a-adb0-4b6f-db16-c00a961777fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics before transformation"
      ],
      "metadata": {
        "id": "Ar5OtyfBM-T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(train['Vector1'], train['Vector2'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(train['Vector1'], train['Vector2'], train['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ],
      "metadata": {
        "id": "jcxNfUfiNI4D",
        "outputId": "d5bba7d0-6dff-4777-ce46-8538fe8a89ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.8144956775260838\n",
            "0.8144956775260832\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.01936280673658871\n",
            "0.014997414103711519\n",
            "-0.01936280673658871\n",
            "0.012208248396559809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics after transformation"
      ],
      "metadata": {
        "id": "iTGM-KbTMwE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oO0zYOBy_K",
        "outputId": "8551720c-2e20-454e-e5d4-ae26fad1f86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity\n",
            "-----------------------------------------------------------------------\n",
            "0.6793048540275171\n",
            "0.679304854027517\n",
            "\n",
            "Correlation\n",
            "-----------------------------------------------------------------------\n",
            "-0.13349611867013872\n",
            "-0.2941803873361792\n",
            "-0.13349611867013872\n",
            "-0.23111734420108906\n"
          ]
        }
      ],
      "source": [
        "print('Similarity')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(np.array(calculate_similarity(transformed, test['Vector1'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY_PAIRWISE)).mean())\n",
        "print(np.array(calculate_similarity(transformed, test['Vector1'],\n",
        "                                    method=Similarity.COSINE_SIMILARITY)).mean())\n",
        "\n",
        "print('\\nCorrelation')\n",
        "print('-----------------------------------------------------------------------')\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.PEARSON_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.SPEARMAN_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.POINT_BISERIAL_CORRELATION))\n",
        "print(calculate_correlation(transformed, test['Vector1'], test['Overall'],\n",
        "                            method=Correlation.KENDALL_TAU_CORRELATION))"
      ]
    }
  ]
}